\section{Previous concepts:}

\subsection{Visio-linguistic:}
Visio-linguistic refers to the intersection and integration of visual and linguistic (language-based) information. It involves understanding and interpreting meaning that is conveyed through a combination of visual elements (images, diagrams, and visual symbols) and linguistic elements (text, spoken words, and written language). This term is particularly relevant in fields such as:

\begin{itemize}
\item \textbf{Cognitive Science and Psychology:} Studying how the human brain processes and integrates visual and linguistic information.
\item \textbf{Computer Science and Artificial Intelligence:} Developing algorithms and systems that can understand and generate multimodal content, such as captioning images, generating descriptive text for videos, or creating systems that can engage in visual question answering (VQA).
\item \textbf{Education:} Using visual aids alongside text to enhance learning and comprehension.
\item \textbf{Communication and Media:} Designing effective ways to convey information using a combination of visual and textual elements, such as infographics, advertisements, and multimedia content.
\end{itemize}
In essence, visio-linguistic approaches consider how visual and linguistic inputs complement each other to enhance understanding and communication.

\subsection{Image-Language models(ILMs):} \label{ilm}
An image-language model is an artificial intelligence system designed to process and understand both visual and textual data, integrating these two modalities to perform various tasks. These models can generate text descriptions from images, produce images from text descriptions, and understand the relationships between visual and textual elements. Key capabilities and applications of image-language models include:

\begin{itemize}
\item \textbf{Image Captioning:} Generating descriptive text for a given image.
\item \textbf{Visual Question Answering (VQA):} Answering questions related to the content of an image.
\item \textbf{Image Generation from Text:} Creating images based on textual descriptions.
\item \textbf{Cross-modal Retrieval:} Finding images that match a given text or vice versa.
\item \textbf{Image-Text Matching:} Evaluating the relevance or similarity between an image and a text description.
\end{itemize}

These models typically combine techniques from computer vision (for processing videos) and natural language processing (for handling text). They often use deep learning architectures such as:

\begin{itemize}
\item \textbf{Convolutional Neural Networks (CNNs):} For extracting features from images.
\item \textbf{Transformers:} For handling and generating text, and sometimes for processing image features.
\item \textbf{Multimodal Models:} Like CLIP (Contrastive Language–Image Pretraining) and DALL-E, developed by OpenAI, which are specifically designed to understand and generate both images and text.
\end{itemize}
\subsection{Video-Language models (VidLMs):} \label{vlm}
Video-language models are advanced machine learning systems designed to understand and generate both video content and associated natural language descriptions. These models can interpret and generate content involving the complex interplay between visual data (videos) and textual data (language). Key capabilities and applications of video-language models:

\begin{itemize}
\item \textbf{Video Captioning:} Automatically generating descriptive text for video content.
\item \textbf{Video Question Answering (Video QA):} Answering questions based on the content of a video.
\item \textbf{Text-to-Video Generation:} Creating video sequences from textual descriptions.
\item \textbf{Video Retrieval:} Finding relevant videos based on textual queries or descriptions.
\item \textbf{Action Recognition:} Identifying and classifying actions depicted in video clips.
\end{itemize}

\noindent These models typically combine techniques from computer vision (for processing videos) and natural language processing (for handling text). Here, there are some Architecture and Models:

\begin{itemize}
\item \textbf{Encoder-Decoder Frameworks:} Commonly used where the encoder processes video frames to create a rich representation and the decoder generates the corresponding textual description.
\item \textbf{Transformer-based Models:} These models leverage attention mechanisms to handle the complexity of video and language data, such as the Vision Transformer (ViT) and variants like VideoBERT.
\item \textbf{Fusion Techniques:} Methods to effectively combine and align visual and textual data, such as cross-modal attention and joint embedding spaces.
\end{itemize}
\subsection{Video-language datasets:}
Video-language datasets are collections of video clips paired with corresponding textual descriptions, annotations, or other language-based data. These datasets are crucial for training and evaluating models in various tasks, including video understanding, captioning, question answering, and more. Here are some notable video-language datasets:

\begin{enumerate}
\item \textbf{MSR-VTT (Microsoft Research Video to Text):}
\begin{itemize}
\item \textbf{Description:} A large-scale video dataset with 10,000 video clips and 200,000 sentences. Each video has 20 sentences describing its content.
\item \textbf{Use Cases:} Video captioning, retrieval, and understanding.
\end{itemize}

\item \textbf{YouCook2:}
\begin{itemize}
\item \textbf{Description:} Contains 2,000 long, unedited videos of cooking activities from YouTube. Each video is annotated with temporal segmentations and textual descriptions of each cooking step.
\item \textbf{Use Cases:} Video segmentation, action recognition, and video captioning.
\end{itemize}

\item \textbf{Charades:}
\begin{itemize}
\item \textbf{Description:} Features 9,848 videos of daily indoor activities, each annotated with multiple textual descriptions and action labels.
\item \textbf{Use Cases:} Activity recognition, video understanding, and human action detection.
\end{itemize}

\item \textbf{ActivityNet Captions:}
\begin{itemize}
\item \textbf{Description:} A subset of the ActivityNet dataset, with 20,000 videos from diverse activities. Each video is paired with temporally localized sentences describing the video content.
\item \textbf{Use Cases:} Dense video captioning, temporal localization, and video summarization.
\end{itemize}

\item \textbf{AVA (Atomic Visual Actions):}
\begin{itemize}
\item \textbf{Description:} Contains 430 15-minute video clips with 80 atomic visual actions annotated per frame.
\item \textbf{Use Cases:} Action detection, video understanding, and temporal action localization.
\end{itemize}
\end{enumerate}

\subsection{Foil:}
Descriptions of the image that are highly similar to the original ones, but contain one single mistake (‘foil word’) \cite{sheknar2017foil}.
%\subsection{Coarse-Grained Representations:} \label{c_grained}
%Coarse-grained representations capture information at a higher level of abstraction, focusing on broader, more general features rather than detailed, specific ones.
%\subsection{Fine-Grained Representations:} \label{f_grained}
%Fine-grained representations capture detailed, specific features of the input data, retaining more granular and nuanced information.

\subsection{MCQ model:} \label{mcq_model}

The MCQ (Multiple-Choice Question) model is a widely used format for assessing knowledge, skills, and abilities in educational settings and beyond. Here’s an explanation of its key aspects:

\begin{itemize}

\item \textbf{Components of MCQ}

\begin{enumerate}

\item \textbf{Stem:} This is the question or problem statement. It provides the context and sets up the scenario for which the examinee must select the correct answer.

\item \textbf{Options:} These are the possible answers provided to the examinee. Typically, there are several options (usually 4-5), and only one of them is correct.

\item \textbf{Key:} This is the correct answer among the options.

\item \textbf{Distractors:} These are the incorrect options designed to distract or mislead examinees who do not know the correct answer. Good distractors are plausible and often based on common misconceptions or errors.

\end{enumerate}

\item \textbf{Types of MCQs}

\begin{enumerate}

\item \textbf{Single Best Answer:} The most common type, where there is only one correct answer.

\item \textbf{True/False:} Simplified MCQs where the examinee must decide whether a statement is true or false.

\item \textbf{Multiple True/False:} Each option must be judged as true or false independently.

\item \textbf{Matching:} Examinees match a set of stems with a set of options.

\item \textbf{Assertion-Reason:} Two statements are given, and the examinee must determine if each is true and if one statement correctly explains the other.

\end{enumerate}

\end{itemize}

